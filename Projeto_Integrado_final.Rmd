---
title: "Projeto Final"
author: "Caio Emerson Tiago Vinicius"
date: '2023-06-17'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r pacotes}
library(tidyverse)
library(ggplot2)
library(summarytools)
library(gmodels)
library(dplyr)
library(fastDummies)
```


```{r leitura}

# nao mostrar os resultados na notacao cientifica
options(scipen = 999)

library(readr)
df <- read_csv("./data/train.csv")
```

```{r}
# Nomes das colunas que você quer renomear
colunas_para_renomear <- setdiff(names(df), c("id", "target"))

# Número de colunas para renomear
num_colunas <- length(colunas_para_renomear)

# Inicializando o dicionário
dicionario <- list()

# Renomear as colunas para x1, x2, x3, ... e criar o dicionario
for (i in 1:num_colunas) {
    colname <- colunas_para_renomear[i]
    new_colname <- paste("x", i, sep="")
    
    # Adicionando ao dicionário
    dicionario[[new_colname]] <- colname
    
    # Renomeando a coluna
    names(df)[names(df) == colname] <- new_colname
}

# Visualizando o dicionario
# dicionario
```



### Mudar as variaveis quantitativas em qualitativas

```{r}
df$x2 <- factor(df$x2)
df$x5 <- factor(df$x5)
df$x27 <- factor(df$x27)
df$x28 <- factor(df$x28)
df$x53 <- factor(df$x53, ordered=TRUE)
df$x59 <- factor(df$x59)
df$x62 <- factor(df$x62)
df$x67 <- factor(df$x67)
df$x78 <- factor(df$x78)
df$x91 <- factor(df$x91)
df$x100 <- factor(df$x100)

```


### Analise descritiva

Conhecer as variaveis

```{r descritiva}
# medidas resumo
summary(df)
```

### Grafico univariado

```{r grafico1}
# grafico de uma variavel quantitativa
qplot(x=df$target)
```

# ```{r grafico_quantitativas}
# qplot(x=df$x1, y=df$target)
# 
# ```


```{r grafico2}
# grafico de uma variavel qualitativa
qplot(x=df$x53, geom = "bar")
```

```{r grafico2}
# grafico de uma variavel qualitativa
qplot(x=df$x61, geom = "bar")
```

```{r grafico5}
boxplot(df$target ~ df$x53)
```

# Separação das variaveis quantitativas, qualitativas e a variavel target

```{r}
dadosQuali <- df %>%
  select(x2,x5,x27,x28,x53,x59,x62,x67,x78,x91,x100)

dadosQuant <- df %>%
  select(-c(id,target,x2,x5,x27,x28,x53,x59,x62,x67,x78,x91,x100))

target <- df$target

```

### Analise descritiva da variavel target

```{r cnt}
summary(target)
```
ntile() é uma função do dplyr para criar categorias usando os quartis.

```{r faixa}

df1 <- df

# Criar a variavel faixa de cnt
df1$fxtarget_num <- ntile(df$target, 4)  
df1$fxtarget_cat <- factor(df1$fxtarget_num, levels=c(1,2,3,4),labels = c(labels=c("[ -0.2398 a 0.5031]", "(0.5031 a 0.9838]",
                                                                                    "(0.9838 a 1.6609]",  "(1.6609 a 61.3261]")))
df1$fxtarget_cat <- factor(df1$fxtarget_cat, ordered =TRUE)
freq(df1$fxtarget_cat)

```

```{r tabela1a}
df1$quali_x53 <- factor(df1$x53)

CrossTable(df1$fxtarget_cat,df1$quali_x53, prop.r = FALSE, prop.c = FALSE, prop.t = FALSE,
           prop.chisq = FALSE,chisq = FALSE)

```
```{r tabela1a}
df1$quali_x5 <- factor(df1$x5)

CrossTable(df1$fxtarget_cat,df1$quali_x5, prop.r = FALSE, prop.c = FALSE, prop.t = FALSE,
           prop.chisq = FALSE,chisq = FALSE)

```
```{r tabela1a}
df1$quali_x28 <- factor(df1$x28)

CrossTable(df1$fxtarget_cat,df1$quali_x28, prop.r = FALSE, prop.c = FALSE, prop.t = FALSE,
           prop.chisq = FALSE,chisq = FALSE)

```

```{r tabela1a}
df1$quali_9a575e82a4 <- factor(df1$x67)

CrossTable(df1$fxtarget_cat,df1$quali_9a575e82a4, prop.r = FALSE, prop.c = FALSE, prop.t = FALSE,
           prop.chisq = FALSE,chisq = FALSE)

```


```{r grafico15}
# frequencia de season
freq_total <- df1 %>%
  select(fxtarget_cat) %>%
  group_by(fxtarget_cat) %>%
  summarise(obs = n()) %>%
  mutate(freq = obs/sum(obs)*100) %>%
  mutate(quali_x53 = ifelse(fxtarget_cat =="[ -0.2398 a 0.5031]", "1",
                               ifelse(fxtarget_cat =="(0.5031 a 0.9838]","2",
                                      ifelse(fxtarget_cat =="(0.9838 a 1.6609]","3","4")))) %>%
  mutate(fxtarget_num=5) %>%
  relocate(fxtarget_num, .before = fxtarget_cat) %>%
  relocate(quali_x53, .after = fxtarget_cat) 

                                                                                   

# frequencia de season por faixa de target
freq_fxtarget <- df1 %>%
  select(fxtarget_num, fxtarget_cat,quali_x53) %>%
  group_by(fxtarget_num, fxtarget_cat,quali_x53) %>%
  summarise(obs = n()) %>%
  mutate(freq = obs/sum(obs)*100)
  
# append  
freq <- rbind(freq_fxtarget,freq_total)  


ggplot(freq, aes(x = fxtarget_num, y = freq, fill = quali_x53, label = round(freq, 1))) +
  geom_col() +
  geom_text(position = position_stack(vjust = 0.5)) +
  labs(title= "Demanda de bikes segundo a estacao do ano",
            x= "Target", y="%") 
  
```

```{r}
# names(dadosQuali)
names(dadosQuant)
```

```{r grafico33}
# grafico de duas variaveis quantitativas 
qplot(x=df$x60, y=df$target)
```
Transformando as variáveis preditoras quantitativas em qualitativas. 
Nesta função utilizamos o método do Intervalo Interquartil (IQR), todas as colunas que apresentaram outliers são convertidas para qualitativas. 
Apos a convesão criamos intervalos para cada uma delas, e o número de bins é definido com base na fórmula de Sturges.

```{r}
# Função para detectar se uma coluna tem outliers
detect_outliers <- function(column) {
  Q1 <- quantile(column, 0.25)
  Q3 <- quantile(column, 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  return(any(column < lower_bound | column > upper_bound))
}

# Função para transformar uma coluna numérica em categorias
transform_to_categories <- function(column) {
  n <- length(column)
  k <- round(1 + 3.322 * log10(n)) # número de bins com base na fórmula de Sturges
  return(cut(column, breaks = k, labels = FALSE))
}
```

Aplicando a função para todos os dados quantitativos

```{r}
# dadosQuant1 = dadosQuant

# Processando cada coluna do dataset
for (col_name in names(dadosQuant)) {
  if (is.numeric(dadosQuant[[col_name]])) {
    if (detect_outliers(dadosQuant[[col_name]])) {
      dadosQuali[[col_name]] <- as.factor(transform_to_categories(dadosQuant[[col_name]]))
      dadosQuant[[col_name]] <- NULL
      cat(sprintf("Transformed column '%s' into categories\n", col_name))
    }
  }
}
```

```{r normalizando}
# Normalização para o intervalo [0, 1]
df_norm <- dadosQuant %>%
  # select(-target) %>%
  mutate(across(everything(), ~(. - min(.)) / (max(.) - min(.))))
```


### Análise de correlacao de Pearson

Escala das Variáveis: Se as variáveis que você está analisando têm escalas muito diferentes, a normalização pode ser útil. Por exemplo, se uma variável é a idade (variando de 0 a 100) e outra é o salário anual (variando de 0 a milhões), os resultados da análise bivariada podem ser afetados pela diferença de escalas.

Distribuição dos Dados: Se os dados têm uma distribuição muito inclinada ou assimétrica, às vezes é útil normalizar ou transformar os dados para fazer com que sua distribuição seja mais próxima da normal. Isso pode ser especialmente importante se você planeja usar métodos estatísticos que assumem uma distribuição normal.

Técnicas de Análise: Algumas técnicas de análise bivariada, como a correlação de Pearson, são sensíveis à escala dos dados. Nesses casos, a normalização pode ser necessária para obter resultados significativos.

Interpretação e Visualização: Em alguns casos, a normalização pode facilitar a interpretação dos resultados e a criação de visualizações mais eficazes.

```{r correlacao}
# selecionar somente as variaveis quantitativas

df_corr <- cbind(df_norm,target)

correlacao <- cor(df_corr)
correlacao

mc <- correlacao
```


```{r grafico13}
library(corrplot)
#corrplot(mc)
corrplot(mc, type="full", method="number")
```

<!-- removendo multicolinearidade -->

<!-- ```{r} -->
<!-- df_norm <- df_norm %>% -->
<!--   select(-c(x18,x20,x22,x50)) -->
<!-- ``` -->

<!-- ```{r correlacao2} -->
<!-- # selecionar somente as variaveis quantitativas -->

<!-- df_corr <- cbind(df_norm,target) -->

<!-- correlacao <- cor(df_corr) -->
<!-- correlacao -->

<!-- mc <- correlacao -->
<!-- ``` -->



<!-- ```{r grafico14} -->
<!-- library(corrplot) -->
<!-- #corrplot(mc) -->
<!-- corrplot(mc, type="full", method="number") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(DescTools) -->
<!-- ``` -->


<!-- ```{r} -->

<!-- # Encontre as colunas que são fatores -->
<!-- factor_cols <- names(dadosQuali)[sapply(dadosQuali, is.factor)] -->

<!-- # Defina o limite de correlação para remoção -->
<!-- threshold <- 0.5 -->

<!-- # Crie uma matriz para armazenar os valores Cramér's V -->
<!-- cor_matrix <- matrix(NA, length(factor_cols), length(factor_cols)) -->
<!-- rownames(cor_matrix) <- factor_cols -->
<!-- colnames(cor_matrix) <- factor_cols -->

<!-- # Calcule Cramér's V para cada par de variáveis categóricas -->
<!-- for (i in 1:length(factor_cols)) { -->
<!--     for (j in 1:length(factor_cols)) { -->
<!--         # Calcula Cramér's V -->
<!--         tabela_contingencia <- table(dadosQuali[[factor_cols[i]]], dadosQuali[[factor_cols[j]]]) -->
<!--         cor_matrix[i, j] <- CramerV(tabela_contingencia) -->
<!--     } -->
<!-- } -->

<!-- # Encontra variáveis altamente correlacionadas -->
<!-- high_cor_vars <- which(cor_matrix > threshold & cor_matrix < 1, arr.ind = TRUE) -->

<!-- # Remova as variáveis altamente correlacionadas -->
<!-- to_remove <- unique(rownames(high_cor_vars)) -->
<!-- dadosQuali1 <- dadosQuali[, !(names(dadosQuali) %in% to_remove)] -->

<!-- # O conjunto de dados 'dados' agora tem as variáveis altamente correlacionadas removidas. -->

<!-- ``` -->




```{r criar dummies}
 
# Criar uma lista de todas as colunas categóricas, exceto 'x5'
# columns_to_dummy <- setdiff(names(dadosQuali), c("x5", "x28", "x67"))

# criar variaveis dummies das variaveis preditoras qualitativas
var_dummies <- dummy_cols(dadosQuali, select_columns = names(dadosQuali),
           remove_first_dummy = TRUE,
           remove_selected_columns = TRUE)
```

```{r target}

df_modelo <- cbind(df_norm,var_dummies,target)

```






### Modelo de Regressão Linear Múltipla
### Dividir a amostra em treino e validação

```{r amostras}

#Dividir em duas amostras
set.seed(1010)
train <- sample(nrow(df_modelo), 0.7*nrow(df_modelo), replace = FALSE)
TrainSet <- df_modelo[train,]
ValidSet <- df_modelo[-train,]

```

### Comparar a variável resposta nas duas amostras

```{r amostra1}
summary(TrainSet$target)

```


```{r amostra2}
summary(ValidSet$target)
```
### Rodar o modelo
```{r modelo}
modelo_inicial <- lm(target ~ ., data = TrainSet)
summary(modelo_inicial)
```
```{r variaveis_significativas }
novo_TrainSet <- TrainSet
novo_ValidSet <- ValidSet
novo_modelo <- modelo_inicial
```


```{r previsao1}

novo_TrainSet$Val_pred <- predict(novo_modelo) 
novo_TrainSet$residuo  <- resid(novo_modelo)
novo_TrainSet$rp <- rstandard(novo_modelo)

```


```{r rmse1}
# Erro quadratico medio na amostra de treino
mse <- mean((novo_TrainSet$target - novo_TrainSet$Val_pred)^2)
sqrt(mse)

```
### Análise dos resíduos do modelo

```{r residuo1}
plot(predict(novo_modelo),novo_TrainSet$residuo, xlab = "Preditor linear",ylab = "Residuos")
abline(h = 0, lty = 2)

```
```{r normalidade1}
qqnorm(residuals(novo_modelo), ylab="Residuos",xlab="Quantis teoricos",main="")
qqline(residuals(novo_modelo))
```


```{r excluir_outlier1}

#Excluir os outliers
TrainSet_1 <-filter(novo_TrainSet,novo_TrainSet$rp>=-2&novo_TrainSet$rp<=2) 

#Pre-processamento dos dados
# Apaga a coluna 
TrainSet_1$Val_pred = NULL
TrainSet_1$residuo = NULL
TrainSet_1$rp = NULL
```

### Rodar o modelo
```{r modelo}
modelo1 <- lm(target ~ ., data = TrainSet_1)
summary(modelo1)
```

```{r previsao2}

TrainSet_1$Val_pred <- predict(modelo1) 
TrainSet_1$residuo  <- resid(modelo1)
TrainSet_1$rp <- rstandard(modelo1)

```

```{r analise2}
df_analise1 <- TrainSet_1 %>%
  select(target, Val_pred, residuo, rp)
```


```{r rmse1}
# Erro quadratico medio na amostra de treino
mse1 <- mean((TrainSet_1$target - TrainSet_1$Val_pred)^2)
sqrt(mse1)

```
### Análise dos resíduos do modelo

```{r residuo2}
plot(predict(modelo1),TrainSet_1$residuo, xlab = "Preditor linear",ylab = "Residuos")
abline(h = 0, lty = 2)

```

```{r normalidade2}
qqnorm(residuals(modelo1), ylab="Residuos",xlab="Quantis teoricos",main="")
qqline(residuals(modelo1))
```
```{r excluir_outlier2}

#Excluir os outliers
TrainSet_2 <-filter(TrainSet_1,TrainSet_1$rp>=-2&TrainSet_1$rp<=2) 

#Pre-processamento dos dados
# Apaga a coluna 
TrainSet_2$Val_pred = NULL
TrainSet_2$residuo = NULL
TrainSet_2$rp = NULL
```

### Rodar o modelo
```{r modelo3}
modelo2 <- lm(target ~ ., data = TrainSet_2)
summary(modelo2)
```

```{r previsao3}

TrainSet_2$Val_pred <- predict(modelo2) 
TrainSet_2$residuo  <- resid(modelo2)
TrainSet_2$rp <- rstandard(modelo2)

```

```{r analise3}
df_analise2 <- TrainSet_2 %>%
  select(target, Val_pred, residuo, rp)
```


```{r rmse3}
# Erro quadratico medio na amostra de treino
mse2 <- mean((TrainSet_2$target - TrainSet_2$Val_pred)^2)
sqrt(mse2)

```
### Análise dos resíduos do modelo

```{r residuo3}
plot(predict(modelo2),TrainSet_2$residuo, xlab = "Preditor linear",ylab = "Residuos")
abline(h = 0, lty = 2)

```

```{r normalidade3}
qqnorm(residuals(modelo2), ylab="Residuos",xlab="Quantis teoricos",main="")
qqline(residuals(modelo2))
```

```{r excluir_outlier3}

#Excluir os outliers
TrainSet_3 <-filter(TrainSet_2,TrainSet_2$rp>=-2&TrainSet_2$rp<=2) 

#Pre-processamento dos dados
# Apaga a coluna 
TrainSet_3$Val_pred = NULL
TrainSet_3$residuo = NULL
TrainSet_3$rp = NULL
```

### Rodar o modelo
```{r modelo4}
modelo3 <- lm(target ~ ., data = TrainSet_3)
summary(modelo3)
```

```{r previsao4}

TrainSet_3$Val_pred <- predict(modelo3) 
TrainSet_3$residuo  <- resid(modelo3)
TrainSet_3$rp <- rstandard(modelo3)

```

```{r analise4}
df_analise3 <- TrainSet_3 %>%
  select(target, Val_pred, residuo, rp)
```


```{r rmse4}
# Erro quadratico medio na amostra de treino
mse3 <- mean((TrainSet_3$target - TrainSet_3$Val_pred)^2)
sqrt(mse3)

```
### Análise dos resíduos do modelo

```{r residuo4}
plot(predict(modelo3),TrainSet_3$residuo, xlab = "Preditor linear",ylab = "Residuos")
abline(h = 0, lty = 2)

```
```{r normalidade4}
qqnorm(residuals(modelo3), ylab="Residuos",xlab="Quantis teoricos",main="")
qqline(residuals(modelo3))
```


##---------------------------------------------------------------------------------------
##                           AMOSTRA DE VALIDACAO
##---------------------------------------------------------------------------------------


```{r amostra_validacao}

# Amostra de validacao

target_pred <- predict(modelo2, interval = "prediction", level = 0.95,
                    newdata = ValidSet, se.fit = T) 

target_pred1 <-target_pred$fit
ValidSet_pred=cbind(ValidSet,target_pred1)
```


```{r residuo_validacao}
# Residuo na amostra de validacao
residuo_valid <- ValidSet_pred$target - ValidSet_pred$fit
hist(residuo_valid)
qqnorm(residuo_valid, ylab="Res?duos",xlab="Quantis te?ricos",main="")
qqline(residuo_valid)
```



```{r rmse_validacao}

# Erro quadratico medio na amostra de validacao
mse2 <- mean((ValidSet_pred$target - ValidSet_pred$fit)^2)
sqrt(mse2)
```
```{r}
library(ggplot2)

# Convertendo as previsões para um data.frame
pred_df <- as.data.frame(target_pred$fit)

# Adicionando a variável independente ou índice ao data.frame (se necessário)
# Substitua variavel_independente pelo nome da sua variável independente ou use um índice sequencial.
pred_df$x <- 1:7493

# Criando o gráfico com apenas as primeiras 10 previsões
ggplot(pred_df[0:50, ], aes(x = x, y = fit)) +
  geom_point(aes(y = ValidSet$target[0:50]), color = 'blue') + # Pontos dos dados reais
  geom_line(aes(y = fit), color = 'black') + # Linha dos valores ajustados
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2) + # Área de intervalo de predição
  labs(x = "Variável Independente", y = "Target", title = "Valores Ajustados com Intervalo de Predição para as Primeiras 10 Observações")


```

```{r}
# Fazer previsões nos dados de validação
predictions <- predict(modelo1, newdata = novo_ValidSet)
```

```{r}
# Calculando o MSE
mse <- mean((novo_ValidSet$target - predictions)^2)

# Calculando o RMSE
rmse <- sqrt(mse)

# Calculando o MAE
mae <- mean(abs(novo_ValidSet$target - predictions))

# Imprimindo os resultados
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")

# Calcular a soma dos quadrados dos resíduos (SS_res)
SS_res <- sum((novo_ValidSet$target - predictions)^2)

# Calcular a soma total dos quadrados (SS_tot)
SS_tot <- sum((novo_ValidSet$target - mean(novo_ValidSet$target))^2)

# Calcular o R² nos dados de validação
R2_validation <- 1 - (SS_res / SS_tot)

# Imprimir o R²
cat("R² nos dados de validação:", R2_validation, "\n")
```

```{r}
# Calcule os erros
errors <- predictions - novo_ValidSet$target_variable

# Calcule a soma dos quadrados dos erros
SSE <- sum(errors^2)

# Calcule a média do target variable nos dados de validação
mean_target <- mean(novo_ValidSet$target_variable)

# Calcule a soma total dos quadrados
SST <- sum((novo_ValidSet$target_variable - mean_target)^2)

# Calcule o R-squared
R_squared <- 1 - (SSE/SST)
R_squared
```



